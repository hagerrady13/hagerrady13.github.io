---
title: "Policy Gradient vs. Action-value based methods"
excerpt: "Short description of portfolio item number 1<br/><img src='/images/ac_vs_sarsa.png'>"
collection: portfolio
---

This project analyzes and compares action-value based methods vs. policy gradient methods, in a control problem. The project suggests that policy gradient methods should
learn faster than action-value based methods under function approximation. More specifically, one-step Sarsa is compared
with one-step actor-critic in the Mountain Car environment, where the empirical results were consistent with the original hypothesis.   
[Report](https://drive.google.com/file/d/1Fb1SBwGZyZXVZk3X79Vh_N-MBvJs0lF6/view?usp=sharing) - [Code](https://github.com/hagerrady13/RLII-project)